---
title: "Stats199_Week3"
author: "Gabriel Fanucchi"
date: "10/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 3: State Space Reconstruction and an Introduction to Chaos

  One of the reasons scientists are interested in dynamical systems is because they can offer a qualitative look at the dynamics without having to solve the underlying equations analyticaly. For example, last week we examined the Lorenz Attractor which was governed by a system of three ODEs. We did not have to solve the equations to understand what a given trajectory is doing at the initial conditions. Instead, we examined the 3 dimensional phase portrait, which showed the famous butterfly curve. A phase portrait is simply a picture that shows all of the qualitatively different trajectories of the system [3]. 
  
In nonlinear time series analysis, we are interested in an approach that will give us the phase portrait without knowing the underlying equations that govern the dynamics. If this is the case, one approach is to collect some time series data and reconstruct the phase portrait. This is done with the help of Takens' Theorem.
  
$\textbf{Theorem}$: (Takens' Embedding Theorem) [1] 
Let $M \subset \mathbb{R^d}$ be a compact manifold of dimension $n$. Let 
$\psi:\mathbb{R} \times M \rightarrow M$ and $f:M \rightarrow \mathbb{R}$ be generic smooth functions. Then, for any $\tau > 0$ the map $M \rightarrow \mathbb{R^{2n+1}}$ defined by $x \mapsto (f(x),f(x_1),f(x_2),...,f(x_{2n}))$ where ${x_i} = \psi(i \cdot \tau, x)$ is an injective map with full rank.

For a simplified version, the theorem says that we can reconstruct the phase portrait in the following procedure [4]:

1. Assume we have a time series with the following measured data: $s_1,s_2,...,s_N$

2. In the m-dimensional reconstructed phase space, a point at time t is given by a vector $y_t$ with the following components:

  first component: $s_t = s(t)$
  
  second component: $s_{t+d} = s(t+d)$
  
  third component: $s_{t+2d} = s(t+2d)$
  
  ...
  
  mth component: $s_{t+(m-1)d} = s(t+(m-1)d)$
  
Note that if m = 2, ${y_t} = (s_t,s_{t+d})$ and if m = 3, ${y_t} = (s_t,s_{t+d},s_{t+2d})$
The vectors of the reconstructed phase space are also called the Takens vectors.

For our embedding parameters, we need to choose m = 3 and d = 1, where m is the embedding dimension and d is called the time delay. The delayed measurements are $s(t+d)$

If N is the length of the time series, the number n of embedding vectors is found by the following equation: $n = N - (m - 1)d$

It's important to note that the embedding theorems like Takens' theorem are so useful because they ensure that the reconstructed phase space defined by $y_t$ is 'equivalent' to the original trajectory defined by $x(t),y(t),$ and $z(t)$.

This theorem is useful for the technique known as attractor reconstruction. With this data analysis technique, the dynamics in the phase portrait can be reconstructed from just one time series [3].

## Examples

### The Lorenz Attractor

The following code below is sourced from [4]:

```{r,fig.height=7}
# Code 3.3   lorenz_attractor.R
#
require(tseriesChaos)
parms<- c(10,8/3,28) # parameters: sigma, beta, rho
tinit<- 0
tfin<- 100
step<- 0.01
times<-seq(tinit,tfin,by=step)
funct<- function(t,integ,parms){  # the system of equations
  x<-integ[1]
  y<-integ[2]
  z<-integ[3]
  sigma<- parms[1]
  beta<- parms[2]
  rho<- parms[3]
  dx<- sigma*(y-x) # that is dx/dt = sigma(y-x)
  dy<- x*(rho-z)-y # that is dy/dt = x(rho-z)-y
  dz<- x*y-beta*z # that is dz/dt = xy -beta z
  list(c(dx,dy,dz))
} # end of funct

require(deSolve)
cinit<-c(1,1,1)
xyz<-lsode(cinit,times,funct,parms)
# xyz # comment if you do not wish the xyz values printed
par(mfrow=c(3,1))
par(mar = c(4.3, 4.8, 1., 3))
par(cex.lab=1.5,cex.axis=1.2,lwd=1,lty=1)
plot(xyz[,1],xyz[,2],type="l",xlab="t",ylab="x(t)",  # x(t) vs t
     xlim=c(tinit,tfin),ylim=c(-30,30))
plot(xyz[,1],xyz[,3],type="l",xlab="t",ylab="y(t)",  # y(t) vs t
     xlim=c(tinit,tfin),ylim=c(-30,30))
plot(xyz[,1],xyz[,4],type="l",xlab="t",ylab="z(t)",  # z(t) vs t
     xlim=c(tinit,tfin),ylim=c(0 ,50))


# phase space portrait
require(scatterplot3d)
scatterplot3d(xyz[,2],xyz[,3],xyz[,4],type="l",xlim=c(-30,30),
cex.lab=1.4,cex.axis=1.2)

trans<- 2000 # integration time step considered as transient
# discard initial transient:
x <- window(xyz[,2],trans)
y <- window(xyz[,3],trans)
z <- window(xyz[,4],trans)
t_start<- trans*step - step # new initial time
t_time<-seq(t_start,tfin,by=step) # new time interval

par(mfrow=c(3,1))
par(mar = c(6.3, 4.8, 1., 3))
par(cex.lab=2,cex.axis=1.6,lwd=1,lty=1)
# x(t), y(t), z(t) after the transient:
plot(t_time,x,type="l",xlab="t",ylab="x(t)",
xlim=c(t_start,tfin),ylim=c(-30,30))
plot(t_time,y,type="l",xlab="t",ylab="y(t)",
xlim=c(t_start,tfin),ylim=c(-30,30))
plot(t_time,z,type="l",xlab="t",ylab="z(t)",
xlim=c(t_start,tfin),ylim=c(0 ,50))
m.max<- 6 # embedding dimensions: from 1 to m_max
d<- 18 # tentative time delay (see below)
tw<- 100 # Theiler window
rt<- 10 # escape factor
eps<- sd(x)/10 # neighbourhood diameter
fn <- false.nearest(x,m.max,d,tw,rt,eps)
fn

plot(fn)

lm<- 60 # largest lag
mutual(x,lag.max = lm) # average mutual information to suggest d

# embedding Procedure
m<- 3 # choose embedding dimension
d<- 18 # choose time delay (d<- 10)
xyz <- embedd(x,m,d) # embed the ’observed’ series

scatterplot3d(xyz, type="l")
```

### The Rossler Attractor

For the Rossler attractor, we have the following:

The following code below is sourced from [4]:

```{r}
library(nonlinearTseries)
library(scatterplot3d)
x <- window(rossler.ts, start=90)
xyz <- embedd(x, m=3, d=8)
scatterplot3d(xyz, type="l")
## embedding multivariate time series
series <- cbind(seq(1,50),seq(101,150))
head(embedd(series, m=6, d=1))
```

## An Alternate Embedding Method - Principal Components Analysis (PCA)

An alternative technique in attractor reconstruction is one that uses principle component analysis (PCA). A rough outline of PCA is explained below.

The following explanation regarding PCA is sourced from [4]:

PCA is an invaluable statistical technique from multivariate statistics that selects the most important variables from a large set of variables that are possibly correlated. Let's say we want to study a particular system. Let "h" be the number of variables. Then, using PCA, we transform the original set into a new set of uncorrelated variables. These are called the principal components. 

From a geometric perspective, this means that we create a new system of orthogonal axes containing the greatest fraction of the total variance of the data. The principal directions are obtained as those eigenvectors of the symmetric autocovariance matrix that correspond to the largest eigenvalues. The variance is small in the remaining orthogonal directions, therefore we can ignore them. These components can then be organized in decreasing order of their contribution to the total variance. So the first component has the maximum variance and the hth component has the minimum. Finally, the k retained componenets account for most of the information present in the data.

Furthermore, there are 11 principal components in this example. Each of the components are linear combinations of the original variables. For example, for the first principal component, we have:
$$ PC1 = z_1w_1 + z_2w_2 + z_3w_3 + ... + z_{11}w_{11}$$
$z_1,...,z_{11}$ are the elements of the first row of the standardized version of the matrix "emb" and $w_1,...,w_{11}$ are the elements of the first column of the loadings matrix. "emb" is the eigenvalues covariance matrix.

Note: the details for obtaining the covariance matrix will not be discussed here.


```{r}
# lorenz_PCA.R
# embedding procedure with the Principal Component Analysis
require(tseriesChaos)
library(nonlinearTseries)
parms<- c(10,8/3,28)  # parameters: sigma, beta, rho
tinit<- 0
tfin<- 100
step<- 0.01
times<-seq(tinit,tfin,by=step)
funct<- function(t,integ,parms){             # the system of equations
  x<-integ[1]
  y<-integ[2]
  z<-integ[3]
  sigma<- parms[1]
  beta<-  parms[2]
  rho<-   parms[3]
  dx<- sigma*(y-x)           # that is  dx/dt = sigma\(y-x)
  dy<- x*(rho-z)-y           # that is  dy/dt = x(rho-z)-y
  dz<- x*y-beta*z            # that is  dz/dt = xy -beta z
  list(c(dx,dy,dz))
}                  # end of funct
require(deSolve)
cinit<-c(1,1,1)
xyz<-lsoda(cinit,times,funct,parms)
trans<- 2000                      # integral time step considered as transient
x <- window(xyz[,2],trans)        # discard  initial transient
t_start<- trans*step - step       # new initial time
t_time<-seq(t_start,tfin,by=step) # new time interval
require(tseriesChaos)
options(digits=5)                 # number of digits to print (only a suggestion)
m<- 11                            # embedding dimension
d<- 1                             # time delay
emb<- embedd(x,m,d)
######################            # up to here it is as in lorenz_attractor.R

# pc<- prcomp(emb)     # the simplified format of the function prcomp()
pc<- prcomp(emb,center=TRUE,scale.=TRUE)  # in a more explicit form
summary(pc)
sd<-pc$sdev
var <- sd^2
var.percent <- var/sum(var)*100
# the scree plot in bar plot form
barplot(var.percent,xlab="Principal Component",ylab="Percent Variance",
        names.arg=1:length(var.percent),                # to label the bars.
        ylim=c(0,100),col="gray",cex.axis=1.2,cex.lab=1.5)
# the scree plot in line plot form
plot(0,0,type="n", xlab="Principal Component", ylab="Percent Variance",ylim=c(0,100),
     xlim=c(1,m),cex.lab=1.5,cex.axis=1.2)
lines(var.percent,col="black",type="b",pch=19,cex=1.4)
# to plot the cumulative fraction of the total variance
cum.var<- cumsum(var.percent[1:m])
lines(cum.var, col="black",type="b",pch=21,cex=1.4)
# to plot the reconstructed attractor
xyz <- predict(pc)[ ,1:3]
require(scatterplot3d)
scatterplot3d(xyz,type="l",cex.lab=1.5,cex.axis=1.2,lab=c(4,4,7),lab.z=4)
# lab() and lab.z: parameters to control the number of tickmarks
```

We can visualize our results of PCA with a scree plot, which is the first plot printed above. The second plot is the line form of the scree plot. For this example, we retain only the first three components and discard the rest. The x axis shows the principal components and the y axis shows the fractions of total variance as percentages.[4]

The value in using PCA to reconstruct the dynamics is that it is an ideal technique for filtering noise. This is because most of the noise is assumed to be contained in the orthogonal directions that we discard.[4]

## Chaos

Recall that some dynamical systems are chaotic. Chaos is a property of some dynamical systems that makes objects like the Lorenz attractor a special mathematical object to study. There is no universal definition of chaos but we can refer to the definition by Steven Strogatz. Robert Devaney has a more mathematical definition.
  
$\textbf{Definition}$: (Chaos) [3]
  $\textit{Chaos}$ occurs when a deterministic system exhibits aperiodic behavior that depends on sensitivity on the initial conditions, thereby rendering long-term prediction impossible.

$\textbf{Definition}$: (Chaos) [5]
  Let $V$ be a set. $f:V\to V$ is said to be chaotic on $V$ if
  
  1. f has sensitive dependence on initial conditions.
  
  2. f is topologically transitive.
  
  3. Periodic points are dense in V.

In other words, the sensitive dependence on initial conditions means that the system has some degree of unpredictibility. Topological transitivity means that the system cannot be broken down into two subsystems. In spite of the unpredictability due to the initial conditions, we also have some regularity given by the dense periodic points. [5]

Some of the motivation for studying nonlinear time series is that we want to analyze chaotic systems, such as the weather, sunspot activity, the stock market, and more. Chaos also appears in the brain and in the heart. Chaos is not good for the heart because it could be signs of atrial fibrillation, or a rapid irregular heartbeat. To a person who has not studied chaos, these systems all seem like disparate systems that merely exhibit randomness. However, they are also linked by the fact that they all exhibit chaos. Perturbing the initial conditions just slightly creates an entirely different weather model. This is why meterologists have difficulty with long term prediction of the weather. Being aware of chaos is helpful for scientists because it puts a limit on our level of predictability.

Next week, we will study the Lyapunov exponent and features of chaos, introduce Poincare's recurrence theorem, the Poincare section, and recurrence plots and networks. Recurrence plots will be useful for translating the phase portrait into a network which we can then perform topological data analysis and machine learning techniques on.